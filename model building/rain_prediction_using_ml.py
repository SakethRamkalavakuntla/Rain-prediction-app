# -*- coding: utf-8 -*-
"""Rain_Prediction using ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xBQCT8lWYKAU1soZGtzOPc2CE8bI_7Aw

#  Rain Prediction using ML

### Time Line of the Project :
- Data Analysis
- Handling Missing Values
- Handling Categorical Varibales
- Feature Engineering
- Model Building using ML
- Model Building using Auto ML i.e PyCaret

### Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn import metrics
import math
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

df=  pd.read_csv("/content/weatherAUS.csv")
pd.set_option("display.max_columns", None)

df

df.nunique()

"""#### Discrete Variable are countable in finit amount of time while numerical variable are to much in number to count"""

# Numerical features
num_var = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Discrete numerical features (few unique values)
discrete_var = [col for col in num_var if df[col].nunique() <= 25]

# Continuous numerical features
cont_var = [col for col in num_var if col not in discrete_var]

# Categorical features (object or category type)
categ_var = df.select_dtypes(include=['object']).columns.tolist()

df[categ_var]

"""### Handling Missing Values"""

df.isnull().sum()

"""the percentage of missing values in each column."""

df.isnull().sum()*100/len(df)

def find_var_type(var):


    if var in discrete_var:
        print("{} is a Numerical Variable, Discrete in nature".format(var))
    elif var in cont_var :
        print("{} is a Numerical Variable, Continuous in nature".format(var))
    else :
        print("{} is a Categorical Variable".format(var))

find_var_type('Cloud3pm')

"""### Ramdom Sample Imputation for the our variables which are having the most percentage of Nul Vlaues"""

def RandomSampleImputation(df, feature):
    # Randomly sample from non-null values
    random_sample = df[feature].dropna().sample(
        df[feature].isnull().sum(), random_state=0, replace=True
    )
    # Align the sampled index to the null index
    random_sample.index = df[df[feature].isnull()].index

    # Fill in the missing values with the sampled values
    df.loc[df[feature].isnull(), feature] = random_sample

RandomSampleImputation(df, "Cloud9am")
RandomSampleImputation(df, "Cloud3pm")
RandomSampleImputation(df, "Evaporation")
RandomSampleImputation(df, "Sunshine")

df.isnull().sum()*100/len(df)

find_var_type('RainToday')

"""###  replace the null values of all the continuous feature which are having less number of null values"""

def MeanImputation(df, feature):
    df[feature]= df[feature]
    mean= df[feature].mean()
    df[feature]= df[feature].fillna(mean)

MeanImputation(df,'Pressure3pm')

MeanImputation(df, 'Pressure9am')
MeanImputation(df, 'MinTemp')
MeanImputation(df, 'MaxTemp')
MeanImputation(df, 'Rainfall')
MeanImputation(df, 'WindGustSpeed')
MeanImputation(df, 'WindSpeed9am')
MeanImputation(df, 'WindSpeed3pm')
MeanImputation(df, 'Pressure9am')
MeanImputation(df, 'Humidity9am')
MeanImputation(df, 'Humidity3pm')
MeanImputation(df, 'Temp3pm')
MeanImputation(df, 'Temp9am')

df.isnull().sum()*100/len(df)

"""### Plotting a HeatMap for the numerical values"""

import matplotlib.pyplot as plt
import seaborn as sns

# Select only numeric columns for correlation
numeric_df = df.select_dtypes(include=['number'])

# Compute Spearman correlation matrix
corrmat = numeric_df.corr(method="spearman")

# Set plot size
plt.figure(figsize=(20, 20))

# Plot heatmap
sns.heatmap(corrmat, annot=True, fmt=".2f", cmap="coolwarm", square=True, cbar_kws={"shrink": 0.5})
plt.title("Spearman Correlation Heatmap", fontsize=16)
plt.show()

"""### Analysis for Continuous variables"""

# Number of plots
n = len(cont_var)
cols = 3  # Number of columns in the grid
rows = math.ceil(n / cols)

# Set figure size
plt.figure(figsize=(cols * 6, rows * 4))

for idx, feature in enumerate(cont_var):
    plt.subplot(rows, cols, idx + 1)
    sns.histplot(data=df, x=feature, kde=True, bins=30)
    plt.xlabel(feature)
    plt.ylabel("Count")
    plt.title(f"Distribution of {feature}")

plt.tight_layout()
plt.show()

# Number of plots
n = len(cont_var)
cols = 3  # Number of columns in the grid
rows = math.ceil(n / cols)

# Set figure size
plt.figure(figsize=(cols * 6, rows * 4))

for idx, feature in enumerate(cont_var):
    plt.subplot(rows, cols, idx + 1)
    sns.boxplot(x=df[feature])
    plt.title(f"Boxplot of {feature}")
    plt.xlabel(feature)

plt.tight_layout()
plt.show()

"""### One Hot Encoding"""

df["RainToday"] = pd.get_dummies(df["RainToday"], drop_first = True)
df["RainTomorrow"] = pd.get_dummies(df["RainTomorrow"], drop_first = True)
df

"""### Lable Encoding"""

for feature in categ_var:
    print(feature, (df.groupby([feature])["RainTomorrow"].mean().sort_values(ascending = False)).index)

windgustdir = {'NNW':0, 'NW':1, 'WNW':2, 'N':3, 'W':4, 'WSW':5, 'NNE':6, 'S':7, 'SSW':8, 'SW':9, 'SSE':10,
       'NE':11, 'SE':12, 'ESE':13, 'ENE':14, 'E':15}
winddir9am = {'NNW':0, 'N':1, 'NW':2, 'NNE':3, 'WNW':4, 'W':5, 'WSW':6, 'SW':7, 'SSW':8, 'NE':9, 'S':10,
       'SSE':11, 'ENE':12, 'SE':13, 'ESE':14, 'E':15}
winddir3pm = {'NW':0, 'NNW':1, 'N':2, 'WNW':3, 'W':4, 'NNE':5, 'WSW':6, 'SSW':7, 'S':8, 'SW':9, 'SE':10,
       'NE':11, 'SSE':12, 'ENE':13, 'E':14, 'ESE':15}
df["WindGustDir"] = df["WindGustDir"].map(windgustdir)
df["WindDir9am"] = df["WindDir9am"].map(winddir9am)
df["WindDir3pm"] = df["WindDir3pm"].map(winddir3pm)

df["WindGustDir"] = df["WindGustDir"].fillna(df["WindGustDir"].value_counts().index[0])
df["WindDir9am"] = df["WindDir9am"].fillna(df["WindDir9am"].value_counts().index[0])
df["WindDir3pm"] = df["WindDir3pm"].fillna(df["WindDir3pm"].value_counts().index[0])

df.isnull().sum()*100/len(df)

df.head()

"""### We have removed all the null values and handeled with categorical data

### Now we will do the Label Encoding for our Location according to our Target variable
"""

df_loc = df.groupby(["Location"])["RainTomorrow"].value_counts().sort_values().unstack()

df_loc.head()

import pandas as pd

row = df.iloc[1]  # This is a Series
numeric_row = pd.to_numeric(row, errors='coerce')  # Convert values to numbers; non-numeric become NaN
numeric_row = numeric_row.dropna()  # Drop NaNs
numeric_row.sort_values(ascending=False)

# Assuming df_loc is grouped by "Location" and then counts of "RainTomorrow" are calculated
df_loc = df.groupby(["Location"])["RainTomorrow"].value_counts().sort_values().unstack()

# # Sorting the counts for the specific location (in this case, 1 refers to the second location, change if needed)
# sorted_values = df_loc.iloc[1].sort_values(ascending=False)

# # Display the sorted values
# print(sorted_values)

df_loc.head()

df_loc[True].sort_values(ascending=False)

df_loc[True].sort_values(ascending=False).index

len(df_loc[True].sort_values(ascending=False))

mapped_location = {'Portland':1, 'Cairns':2, 'Walpole':3, 'Dartmoor':4, 'MountGambier':5,
       'NorfolkIsland':6, 'Albany':7, 'Witchcliffe':8, 'CoffsHarbour':9, 'Sydney':10,
       'Darwin':11, 'MountGinini':12, 'NorahHead':13, 'Ballarat':14, 'GoldCoast':15,
       'SydneyAirport':16, 'Hobart':17, 'Watsonia':18, 'Newcastle':19, 'Wollongong':20,
       'Brisbane':21, 'Williamtown':22, 'Launceston':23, 'Adelaide':24, 'MelbourneAirport':25,
       'Perth':26, 'Sale':27, 'Melbourne':28, 'Canberra':29, 'Albury':30, 'Penrith':31,
       'Nuriootpa':32, 'BadgerysCreek':33, 'Tuggeranong':34, 'PerthAirport':35, 'Bendigo':36,
       'Richmond':37, 'WaggaWagga':38, 'Townsville':39, 'PearceRAAF':40, 'SalmonGums':41,
       'Moree':42, 'Cobar':43, 'Mildura':44, 'Katherine':45, 'AliceSprings':46, 'Nhil':47,
       'Woomera':48, 'Uluru':49}
df["Location"] = df["Location"].map(mapped_location)

"""### Mapping Data"""

# df["Date"] = pd.to_datetime(df["Date"], format = "%Y-%m-%dT", errors = "coerce")
# df["Date_month"] = df["Date"].dt.month
# df["Date_day"] = df["Date"].dt.day

df["Date"] = pd.to_datetime(df["Date"], format="%Y-%m-%d", errors="coerce")
df["Date_month"] = df["Date"].dt.month
df["Date_day"] = df["Date"].dt.day

df.head()

sns.countplot(x="RainTomorrow", data=df)
plt.title("Rain Tomorrow Count")
plt.xlabel("Rain Tomorrow")
plt.ylabel("Count")
plt.show()

df= df.drop(['Date'],axis=1)

df.head()

"""### Plotting Q-Q Plot"""

import scipy.stats as stats
import pylab

def plot_curve_grid(df, cont_var, rows=5, cols=4):
    fig, axes = plt.subplots(rows * 2, cols, figsize=(cols * 5, rows * 4))
    axes = axes.flatten()

    for i, feature in enumerate(cont_var):
        # Histogram
        axes[2 * i].hist(df[feature].dropna(), bins=30, color='skyblue')
        axes[2 * i].set_title(f'{feature} - Histogram')

        # Q-Q plot
        stats.probplot(df[feature].dropna(), dist="norm", plot=axes[2 * i + 1])
        axes[2 * i + 1].set_title(f'{feature} - Q-Q Plot')

    # Hide any unused subplots
    for j in range(2 * len(cont_var), len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.show()

# Call the function
plot_curve_grid(df, cont_var)

"""### Splitting the data"""

x = df.drop(["RainTomorrow"], axis=1)
y = df["RainTomorrow"]

from sklearn.preprocessing import StandardScaler

scale=StandardScaler()

scale.fit(x)

X= scale.transform(x)

x.columns

X=pd.DataFrame(X,columns=x.columns)

X.head()

y.head()

X_train, X_test, y_train, y_test = train_test_split(x,y, test_size =0.2, random_state = 0)

"""## Model Building using ML Models.
- RandomForestClassifier
- GaussianNB
- KNeighborsClassifier
- XGB Classifier

## Random Forest Classifier
"""

from sklearn.ensemble import RandomForestClassifier

ranfor= RandomForestClassifier()

ranfor.fit(X_train,y_train)

ypred= ranfor.predict(X_test)

print(confusion_matrix(y_test,ypred))
print(accuracy_score(y_test,ypred))
print(classification_report(y_test,ypred))

from sklearn.metrics import RocCurveDisplay, roc_auc_score

# Plot ROC Curve
RocCurveDisplay.from_estimator(ranfor, X_test, y_test)

# Show the plot
plt.show()

# Compute AUC Score
print("AUC Score:", roc_auc_score(y_test, ypred))

"""## Gaussian NB"""

from sklearn.naive_bayes import GaussianNB

gnb= GaussianNB()

gnb.fit(X_train,y_train)

ypred2= gnb.predict(X_test)

print(confusion_matrix(y_test,ypred2))
print(accuracy_score(y_test,ypred2))
print(classification_report(y_test,ypred2))

from sklearn.metrics import RocCurveDisplay, roc_auc_score

# Plot ROC Curve
RocCurveDisplay.from_estimator(gnb, X_test, y_test)

# Show the plot
plt.show()

# Compute AUC Score
print("AUC Score:", roc_auc_score(y_test, ypred2))

"""## K Nearest Neighbors"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)

knn.fit(X_train,y_train)

ypred3= knn.predict(X_test)

print(confusion_matrix(y_test,ypred3))
print(accuracy_score(y_test,ypred3))
print(classification_report(y_test,ypred3))

from sklearn.metrics import RocCurveDisplay, roc_auc_score

# Plot ROC Curve
RocCurveDisplay.from_estimator(knn, X_test, y_test)

# Show the plot
plt.show()

# Compute AUC Score
print("AUC Score:", roc_auc_score(y_test, ypred3))

"""## XGB Classifier"""

from xgboost import XGBClassifier

xgb= XGBClassifier()

xgb.fit(X_train,y_train)

ypred4= xgb.predict(X_test)

print(confusion_matrix(y_test,ypred4))
print(accuracy_score(y_test,ypred4))
print(classification_report(y_test,ypred4))

from sklearn.metrics import RocCurveDisplay, roc_auc_score

# Plot ROC Curve
RocCurveDisplay.from_estimator(xgb, X_test, y_test)

# Show the plot
plt.show()

# Compute AUC Score
print("AUC Score:", roc_auc_score(y_test, ypred4))

"""###  save the best performing model i.e. XGB Classsifier model in our pickle file"""

import pickle

# Save the model
with open("rain_XGBnew_model.pkl", "wb") as file:
    pickle.dump(xgb, file)

# Load the model
with open("rain_XGBnew_model.pkl", "rb") as file:
    model = pickle.load(file)